# -*- coding: utf-8 -*-
"""Mesure de la satisfaction client.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x0nzR0tvxieraQVoHYWLQuyTnNi1zht0

# Importation
"""

import pandas as pd
import numpy as np

df = pd.read_json("/content/ensai2023_mode_responsable.json")

df.head()

import json

with open('/content/ensai2023_mode_responsable.json') as mon_fichier:
    data = json.load(mon_fichier)

from pandas import json_normalize
df = json_normalize(data['results']) #Results contain the required data
df.head()

df.columns

df.trending

target_columns = ["content", "votes", "author.age", "author.postalCode"]

df_target = df[target_columns]
df_target.head()

votes = json_normalize(df['votes'])

votes = df['votes'].apply(pd.Series)

sous_votes_agree = votes.iloc[:, 0].apply(pd.Series)
sous_votes_disagree = votes.iloc[:, 1].apply(pd.Series)
sous_votes_neutral = votes.iloc[:, 2].apply(pd.Series)

agree_count = list(sous_votes_agree["count"])
disagree_count = list(sous_votes_disagree["count"])
neutral_count = list(sous_votes_neutral["count"])

agree_qualifications = sous_votes_agree['qualifications'].apply(pd.Series)

agree_likeIt = agree_qualifications.iloc[:,0].apply(pd.Series)
agree_doable = agree_qualifications.iloc[:,1].apply(pd.Series)
agree_platitude = agree_qualifications.iloc[:,2].apply(pd.Series)

agree_count_likeIt = list(agree_likeIt["count"])
agree_count_doable = list(agree_doable["count"])
agree_count_platitude = list(agree_platitude["count"])

disagree_qualifications = sous_votes_neutral['qualifications'].apply(pd.Series)
print(disagree_qualifications)

disagree_qualifications = sous_votes_disagree['qualifications'].apply(pd.Series)

disagree_noWay = disagree_qualifications.iloc[:,0].apply(pd.Series)
disagree_impossible = disagree_qualifications.iloc[:,1].apply(pd.Series)
disagree_platitude = disagree_qualifications.iloc[:,2].apply(pd.Series)

disagree_count_noWay = list(disagree_noWay["count"])
disagree_count_impossible = list(disagree_impossible["count"])
disagree_count_platitude = list(disagree_platitude["count"])


neutral_qualifications = sous_votes_neutral['qualifications'].apply(pd.Series)

neutral_doNotUnderstand = neutral_qualifications.iloc[:,0].apply(pd.Series)
neutral_noOpinion = neutral_qualifications.iloc[:,1].apply(pd.Series)
neutral_doNotCare = neutral_qualifications.iloc[:,2].apply(pd.Series)

neutral_count_doNotUnderstand = list(neutral_doNotUnderstand["count"])
neutral_count_noOpinion = list(neutral_noOpinion["count"])
neutral_count_doNotCare = list(neutral_doNotCare["count"])

df_wqual = df_target.assign(agree = agree_count, agree_count_likeIt = agree_count_likeIt, agree_count_doable= agree_count_doable,agree_count_platitude =agree_count_platitude , disagree = disagree_count, disagree_noWay = disagree_count_noWay, disagree_impossible = disagree_count_impossible, disagree_platitude =disagree_count_platitude, neutral = neutral_count, neutral_doNotUnderstand= neutral_count_doNotUnderstand, neutral_noOpinion = neutral_count_noOpinion, neutral_count_doNotCare= neutral_count_doNotCare )

df_wqual.head()

df_wqual = df_wqual.drop(votes)

df_wqual.to_csv("dw_wqual")

"""# Nettoyage"""

pip install nltk

pip install spacy

!python -m spacy download fr_core_news_md

!pip install zeugma &> /dev/null

pip install tensorflow

import numpy as np
import json
import spacy
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd

mots = set(line.strip() for line in open('/content/dictionnaire.txt'))

nlp = spacy.load('fr_core_news_md')

df = pd.read_csv('/content/dw_wqual')

df.head()

import nltk
nltk.download('punkt')

sentence_list = []
lemma_list = []
lemma_sentences = []

for content in df['content']:
    doc = nlp(content)  # Traitement du texte avec spaCy
    lemma = []
    for token in doc:
        lemma.append(token.lemma_)
    lemma_list.append(lemma)
    sentence_list.append(doc.text)
    lemma_sentence = ' '.join(lemma)
    lemma_sentences.append(lemma_sentence)

print(lemma_sentences)

from spacy.lang.fr.stop_words import STOP_WORDS
stop = STOP_WORDS
print(stop)
stop_perso = ['falloir', '’']

import string

def French_Preprocess_listofSentence(listofSentence):
 preprocess_list = []
 for sentence in listofSentence :
  sentence_w_punct = "".join([i.lower() for i in sentence if i not in string.punctuation])

  sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())

  tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)

  words_w_stopwords = [i for i in tokenize_sentence if i not in stop and i not in stop_perso]

  #words_lemmatize = [lemmatizer.lemmatize(w) for w in words_w_stopwords]

  sentence_clean = ' '.join(w for w in words_w_stopwords if w.lower() in mots or not w.isalpha())

  preprocess_list.append(sentence_clean)

 return preprocess_list

french_preprocess_list = French_Preprocess_listofSentence(lemma_sentences)

french_preprocess_list

from collections import Counter

text = ' '.join(french_preprocess_list)
words = text.split()
word_counts = Counter(words)

top_10_words = word_counts.most_common(10)
for word, count in top_10_words:
    print(word, count)

len(words)

import gensim
from gensim.models import Word2Vec

tokenized_sentences = [sentence.split() for sentence in french_preprocess_list]

model = Word2Vec(tokenized_sentences, vector_size = 30, min_count=5,workers=5, window = 5, sg = 0)
model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=250)

"""size : La dimension du vecteur créé, idéalement inférieur au nombre de mots du vocabulaire

fenêtre : La distance maximale entre un mot cible et les mots autour du mot cible. La fenêtre par défaut est de 5.

min_count : Le nombre minimum de mots à prendre en compte lors de l’apprentissage du modèle ; les mots dont l’occurrence est inférieure à ce nombre seront ignorés. La valeur par défaut de min_count est 5.

Nombre de threads (workers) : Le paramètre workers contrôle le nombre de threads utilisés pour l'entraînement du modèle. Il est généralement recommandé de définir ce nombre en fonction du nombre de cœurs de processeur disponibles sur votre machine. Vous pouvez expérimenter avec différentes valeurs pour trouver un équilibre entre vitesse d'entraînement et utilisation des ressources. Par défaut il y en a 3
"""

similar_words = model.wv.most_similar('cuir', topn=5)
for word, similarity in similar_words:
    print(word, similarity)

from sklearn.manifold import TSNE
import numpy as np
import matplotlib.pyplot as plt

def display_closestwords_tsnescatterplot_perso(model, word):
    arr = np.empty((0, model.vector_size), dtype='f')
    word_labels = [word]

    numb_sim_words = 10

    close_words = model.wv.most_similar(word)[:numb_sim_words]

    arr = np.append(arr, np.array([model.wv[word]]), axis=0)
    for wrd_score in close_words:
        wrd_vector = model.wv[wrd_score[0]]
        word_labels.append(wrd_score[0])
        arr = np.append(arr, np.array([wrd_vector]), axis=0)

    tsne = TSNE(n_components=2, random_state=0, perplexity=5)  # Decreased perplexity value
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)

    x_coords = Y[:, 0]
    y_coords = Y[:, 1]

    color = ['red']
    for i in range(numb_sim_words):
        color.append('blue')

    plt.scatter(x_coords, y_coords, c=color)

    for label, x, y in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x, y), xytext=(1, 5), textcoords='offset points')

    plt.xlim(min(x_coords) - 100, max(x_coords) + 100)
    plt.ylim(min(y_coords) - 100, max(y_coords) + 100)
    plt.show()

    print("Word most similar to: " + word)
    print([sim_word[0] for sim_word in close_words])

display_closestwords_tsnescatterplot_perso(model, tokenized_sentences[0][0])

#vectorisation des phrases via vecteur moyen

df['french_preprocess_list'] = french_preprocess_list

df.head()

def tokenize_text(text):
    tokens = nltk.word_tokenize(text)
    return tokens

df['tokens'] = df['french_preprocess_list'].apply(tokenize_text)

df.head()

df.to_csv('df_wtoken.csv')

import numpy as np

def calculate_average_vector(tokens):
    vectors = [model.wv[token] for token in tokens if token in model.wv]
    if len(vectors) > 0:
        average_vector = np.mean(vectors, axis=0)
        return average_vector
    else:
        return None

df['average_vector'] = df['tokens'].apply(calculate_average_vector)

df.head()

df.to_csv('df_wavgvector.csv')

import numpy as np

null_indices = np.where(df.average_vector.isnull())[0]

print("Indices des lignes avec des valeurs None :", null_indices)

import pandas as pd

null_rows = df.iloc[null_indices]

null_rows.head()

df_drop = df.drop('votes', axis = 1)

df_cleaned = df_drop.dropna(subset=['average_vector'])

df_cleaned

from sklearn.manifold import TSNE
import numpy as np

vectors = np.array(df_cleaned['average_vector'].tolist())

tsne = TSNE(n_components=3, random_state=0, perplexity=5)
vectors_3d = tsne.fit_transform(vectors)

df_cleaned['average_vector_3d'] = vectors_3d.tolist()

print(df_cleaned)

pip --version sklearn

df_cleaned.head()

from sklearn.cluster import KMeans

vectors = np.array(df_cleaned.average_vector_3d.tolist())

k = 4

kmeans = KMeans(n_clusters=k)
kmeans_fit = kmeans.fit(vectors)
#cluster_labels = kmeans.fit(vectors)

print(kmeans_fit.labels_)

df_cleaned['clusters_labels'] = kmeans_fit.labels_

df_cleaned.head()

#Choix du nombre de clusters k (on fait varier k et on surveille l'évolution de l'inertie intra classes)
tab = []

vectors = np.array(df_cleaned.average_vector_3d.tolist())

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i)
    kmeans.fit(vectors)
    tab.append(kmeans.inertia_)

plt.plot(range(1, 11), tab)
plt.title("Méthode Eblow / Méthode du coude")
plt.xlabel("Nombre de clusters")
plt.ylabel("Inertie de l'intra-classe")
plt.show()
#Le nombre optimal de clusters est le point représentant le coude. C'est le point à partir duquel la variance ne se réduit plus

import plotly.graph_objects as go

# Créer une figure 3D
fig = go.Figure()

# Parcourir chaque cluster et ses vecteurs
for label in range(k):
    cluster_indices = np.where(cluster_labels == label)[0]
    cluster_data = cluster_vectors[cluster_indices]

    # Extraire les coordonnées x, y, z des vecteurs
    x_coords = cluster_data[:, 0]
    y_coords = cluster_data[:, 1]
    z_coords = cluster_data[:, 2]

    # Ajouter les points du cluster à la figure
    fig.add_trace(go.Scatter3d(
        x=x_coords,
        y=y_coords,
        z=z_coords,
        mode='markers',
        marker=dict(
            size=3,
            opacity=0.8
        ),
        name=f'Cluster {label}'
    ))

# Définir les axes
fig.update_layout(scene=dict(
    xaxis=dict(title='Dimension 1'),
    yaxis=dict(title='Dimension 2'),
    zaxis=dict(title='Dimension 3')
))

# Afficher la figure interactive
fig.show()

df_cleaned.head()

from collections import Counter

cluster_words = {}

for label in range(k):
    cluster_indices = np.where(df_cleaned['clusters_labels'] == label)[0]
    cluster_sentences = df_cleaned.iloc[cluster_indices]['french_preprocess_list'].tolist()

    word_counter = Counter()
    for sentence in cluster_sentences:
        words = sentence.split()
        word_counter.update(words)

    cluster_words[label] = [word for word, count in word_counter.most_common(30)]

for label, words in cluster_words.items():
    print(f"Cluster {label}:")
    print(words)
    print()

cluster_representative_sentences = {}

for label in range(k):
    cluster_indices = np.where(df_cleaned['clusters_labels'] == label)[0]
    cluster_sentences = df_cleaned.iloc[cluster_indices]['french_preprocess_list'].tolist()

    center_distance = kmeans_fit.transform(vectors[cluster_indices])[:, label]
    representative_sentence_index = np.argmin(center_distance)

    representative_sentence = cluster_sentences[representative_sentence_index]

    cluster_representative_sentences[label] = representative_sentence

for label, sentence in cluster_representative_sentences.items():
    print(f"Cluster {label}: {sentence}")

# Créer un dictionnaire pour stocker les phrases les plus proches du centre de chaque cluster
cluster_closest_sentences = {}

# Parcourir chaque cluster
for label in range(k):
    # Filtrer les phrases appartenant au cluster actuel
    cluster_indices = np.where(df_cleaned['clusters_labels'] == label)[0]
    cluster_sentences = df_cleaned.iloc[cluster_indices]['french_preprocess_list'].tolist()

    # Calculer les distances des vecteurs au centre du cluster
    distances = kmeans_fit.transform(vectors[cluster_indices])

    # Récupérer les indices des 10 phrases les plus proches
    closest_sentence_indices = np.argsort(distances[:, label])[:10]

    # Récupérer les phrases les plus proches
    closest_sentences = [cluster_sentences[i] for i in closest_sentence_indices]

    # Stocker les phrases dans le dictionnaire
    cluster_closest_sentences[label] = closest_sentences

# Afficher les 10 phrases les plus proches du centre de chaque cluster
for label, sentences in cluster_closest_sentences.items():
    print(f"Cluster {label}:")
    for sentence in sentences:
        print(sentence)
    print()

best_dbscan = DBSCAN(eps = 8, min_samples= 100)
best_dbscan.fit(vectors)
cluster_labels_dbscan = best_dbscan.labels_

best_dbscan = DBSCAN(eps = 5.6, min_samples= 50)
best_dbscan.fit(vectors)
cluster_labels_dbscan = best_dbscan.labels_
unique_labels = np.unique(cluster_labels_dbscan)

import matplotlib.pyplot as plt

# Obtenir les étiquettes de cluster uniques
unique_labels = np.unique(cluster_labels_dbscan)
plt.figure(figsize=(10,8))
# Afficher les points de chaque cluster avec une couleur différente
for label in unique_labels:
    if label == -1:
        # Points du bruit (outliers)
        cluster_points = vectors[cluster_labels_dbscan == label]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c='gray', label='Noise')
    else:
        # Points du cluster
        cluster_points = vectors[cluster_labels_dbscan == label]
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {label}')

plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.legend()
plt.show()

cluster_words = {}
cluster_sentences = {}

# Parcourir chaque cluster
for label in unique_labels:
    if label == -1:
        continue

    # Filtrer les phrases appartenant au cluster actuel
    cluster_indices = np.where(cluster_labels_dbscan == label)[0]
    cluster_texts = df_cleaned.iloc[cluster_indices]['french_preprocess_list'].tolist()

    # Compter la fréquence des mots dans le cluster
    word_counter = Counter()
    for text in cluster_texts:
        words = text.split()
        word_counter.update(words)

    # Extraire les 30 mots les plus fréquents du cluster
    cluster_words[label] = [word for word, count in word_counter.most_common(30)]

    # Stocker les phrases représentatives du cluster
    cluster_sentences[label] = cluster_texts[:20]

for label, words in cluster_words.items():
    print(f"Cluster {label}:")
    print(words)
    print()

for label, sentences in cluster_sentences.items():
    print(f"Cluster {label}:")
    for i, sentence in enumerate(sentences, 1):
        print(f"Phrase {i}: {sentence}")
    print()